{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc11765e-e881-4f59-a038-badfb5131eec",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h1>Projet DLA</h1>\n",
    "    <h2>Style Transfer Using Convolutional Neural Network</h2> \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e695e2e7-05db-487a-8094-0b21a2aaa5f2",
   "metadata": {},
   "source": [
    "## Import modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb3630b-85ec-4f8b-b40a-6f93af0b08e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from torchvision.models import vgg19, VGG19_Weights\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ce92e2-400f-4b1f-8566-b5c778b0065d",
   "metadata": {},
   "source": [
    "## Load and display images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fae5f8-b27f-4bc9-8e4e-fd9e5a216ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use cuda if it is available for GPU training otherwise it will use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# shape of the output image\n",
    "imshape = (224, 224)\n",
    "\n",
    "def image_loader(image_name):\n",
    "    # scale imported image\n",
    "    # transform it into a torch tensor\n",
    "    loader = transforms.Compose([transforms.Resize(imshape),  transforms.ToTensor()])\n",
    "\n",
    "    image = Image.open(image_name)\n",
    "    image = loader(image).unsqueeze(0)   # add an additional dimension for fake batch (here 1)\n",
    "    return image.to(device, torch.float) # move the image tensor to the correct device\n",
    "\n",
    "def image_display(tensor, title=None):\n",
    "    unloader = transforms.ToPILImage()  # reconvert into PIL image\n",
    "    image = tensor.cpu().clone()        # clone the tensor\n",
    "    image = unloader(image.squeeze(0))  # remove the fake batch dimension\n",
    "    plt.show()\n",
    "    plt.imshow(image)\n",
    "    if title is not None:\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a987292-efbe-40cd-8cf9-6a7328bedb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of the output image\n",
    "imshape = (224, 224)\n",
    "\n",
    "image_path = \"../data/\"\n",
    "content_image_name = \"content.jpeg\"\n",
    "style_image_name = \"style.jpeg\"\n",
    "\n",
    "content_image = image_loader(image_path + content_image_name)\n",
    "style_image = image_loader(image_path + style_image_name)\n",
    "\n",
    "content_height, content_width = content_image.shape[2], content_image.shape[3]\n",
    "style_height, style_width = style_image.shape[2], style_image.shape[3]\n",
    "\n",
    "print(f\"Content image shape : {content_height} x {content_width}\")\n",
    "print(f\"Style image shape : {style_height} x {style_width}\")\n",
    "print(content_image.size())\n",
    "image_display(content_image, \"Content image\")\n",
    "image_display(style_image, \"Style image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff47d0b-df94-4bc9-a25c-6997007ac4ea",
   "metadata": {},
   "source": [
    "### Load the VGG-19 pretrained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c7e004-de12-4144-855c-d085af545626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the VGG 19 model with pre-trained weights\n",
    "model = models.vgg19(weights=VGG19_Weights.IMAGENET1K_V1).to(device) # move the model to the correct device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9103d523-a570-4f5d-ba71-9b8776976d83",
   "metadata": {},
   "source": [
    "<h2>1. Choice of the layers for the content representation</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dad6c7-9a03-490c-b343-f2ed235b3f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_layers_test = [\"conv1_2\",\"conv2_2\",\"conv3_2\",\"conv4_2\",\"conv5_2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d1acaf-82f2-4862-bfc9-fd149b503e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = [2, 2, 4, 4, 4]  # Number of convolutional layers in each block of the VGG-19 model\n",
    "renamed_model = nn.Sequential()\n",
    "\n",
    "# Renommer les couches\n",
    "index_conv = 0\n",
    "index_relu = 0\n",
    "current_block = 0\n",
    "i = 0\n",
    "\n",
    "for layer in model.features.eval().children():\n",
    "    # Vérifier si on doit passer au bloc suivant\n",
    "    if current_block < len(blocks) and index_conv == blocks[current_block]:\n",
    "        index_conv = 0  # Réinitialiser le compteur pour le nouveau bloc\n",
    "        current_block += 1\n",
    "\n",
    "    if isinstance(layer, nn.Conv2d):  # Pour les couches convolutionnelles\n",
    "        index_conv += 1\n",
    "        name = f'conv{current_block + 1}_{index_conv}'\n",
    "        renamed_model.add_module(name, layer)\n",
    "\n",
    "    elif isinstance(layer, nn.ReLU):  # Pour les couches ReLU\n",
    "        index_relu += 1\n",
    "        name = f'relu{current_block + 1}_{index_relu}'\n",
    "        renamed_model.add_module(name, nn.ReLU(inplace=False))  # Utiliser inplace=False pour compatibilité\n",
    "\n",
    "    elif isinstance(layer, nn.MaxPool2d):  # Pour les couches MaxPooling\n",
    "        name = f'pool{current_block + 1}'\n",
    "        renamed_model.add_module(name, layer)\n",
    "    i += 1\n",
    "# Display the name of the layers\n",
    "print(renamed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131d57c1-4855-4010-8965-944d896f6754",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGActivations_content(nn.Module):\n",
    "    \"\"\"\n",
    "    Extracts activations from specific layers of a model for content reconstruction.\n",
    "\n",
    "    Attributes:\n",
    "    ------------\n",
    "    model : nn.Module\n",
    "        The pretrained model (e.g., VGG) used to extract activations.\n",
    "    target_layers : list of str\n",
    "        The names of the layers from which to extract activations.\n",
    "    layer_outputs : dict\n",
    "        A dictionary that stores the activations for the target layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, target_layers):\n",
    "        \"\"\"\n",
    "        Initializes the class with the given model and the list of target layers.\n",
    "\n",
    "        Parameters:\n",
    "        ------------\n",
    "        model : nn.Module\n",
    "            The pretrained model used to extract activations.\n",
    "        target_layers : list of str\n",
    "            The names of the layers from which to extract activations.\n",
    "        \"\"\"\n",
    "        super(VGGActivations_content, self).__init__()\n",
    "        self.model = model\n",
    "        self.target_layers = target_layers\n",
    "        self.layer_outputs = {}\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Passes input through the model and extracts activations for the target layers.\n",
    "\n",
    "        Parameters:\n",
    "        ------------\n",
    "        x : torch.Tensor\n",
    "            The input image tensor.\n",
    "\n",
    "        Returns:\n",
    "        ------------\n",
    "        dict\n",
    "            A dictionary containing activations for the specified target layers.\n",
    "        \"\"\"\n",
    "        self.layer_outputs = {}  # Reset the output dictionary\n",
    "        for name, layer in self.model.named_children():\n",
    "            x = layer(x)  # Pass the input through each layer\n",
    "            if name in self.target_layers:\n",
    "                self.layer_outputs[name] = x  # Store activations for target layers\n",
    "        return self.layer_outputs\n",
    "\n",
    "\n",
    "def reconstruct_image_content(activations_dict):\n",
    "    \"\"\"\n",
    "    Reconstructs an image from the activations of specific layers using optimization.\n",
    "\n",
    "    Parameters:\n",
    "    ------------\n",
    "    activations_dict : dict\n",
    "        A dictionary containing activations for the target layers.\n",
    "\n",
    "    Returns:\n",
    "    ------------\n",
    "    dict\n",
    "        A dictionary mapping each layer to the reconstructed image.\n",
    "    \"\"\"\n",
    "    # Dictionary to store reconstructed images for each layer\n",
    "    reconstructed_images = {}\n",
    "\n",
    "    # Iterate over each target layer and its corresponding activation\n",
    "    for layer, activation in activations_dict.items():\n",
    "        print(f\"{'-'*10} Running optimization for layer: {layer} {'-'*10}\")\n",
    "\n",
    "        # Initialize a random image for optimization\n",
    "        reconstructed_image = torch.rand_like(input_image, requires_grad=True)\n",
    "\n",
    "        # Set up an optimizer to update the random image\n",
    "        optimizer = torch.optim.Adam([reconstructed_image], lr=0.01)\n",
    "\n",
    "        # Perform optimization to reconstruct the image\n",
    "        for step in range(3000):\n",
    "            optimizer.zero_grad()  # Reset gradients\n",
    "\n",
    "            # Compute activations for the current reconstructed image\n",
    "            generated_activations = vgg_activations_content(reconstructed_image)\n",
    "\n",
    "            # Compute the loss between the generated and target activations\n",
    "            loss = torch.nn.functional.mse_loss(generated_activations[layer], activations_dict[layer])\n",
    "\n",
    "            loss.backward()  # Backpropagate the loss\n",
    "            optimizer.step()  # Update the image based on gradients\n",
    "\n",
    "            # Log progress every 50 steps\n",
    "            if step % 50 == 0:\n",
    "                print(f\"Step {step} - Loss: {loss.item()}\")\n",
    "\n",
    "        # Store the optimized image for the current layer\n",
    "        reconstructed_images[layer] = reconstructed_image\n",
    "\n",
    "    return reconstructed_images\n",
    "\n",
    "\n",
    "def deprocess(tensor):\n",
    "    \"\"\"\n",
    "    Dénormalizes an image tensor for visualization.\n",
    "\n",
    "    Parameters:\n",
    "    ------------\n",
    "    tensor : torch.Tensor\n",
    "        A normalized image tensor (e.g., in the range of -1 to 1).\n",
    "\n",
    "    Returns:\n",
    "    ------------\n",
    "    torch.Tensor\n",
    "        A denormalized image tensor (in the range of 0 to 1, clamped).\n",
    "    \"\"\"\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)  # Mean for ImageNet normalization\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)    # Std for ImageNet normalization\n",
    "    tensor = tensor * std + mean  # Denormalize\n",
    "    return tensor.clamp(0, 1)  # Clamp values to ensure valid range for visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a789123e-4e11-41ea-afd7-6bda85c7c387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prétraitement pour les images VGG\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "input_image = preprocess(torchvision.transforms.functional.to_pil_image(content_image.squeeze(0))).unsqueeze(0)\n",
    "\n",
    "# Instantiate the new model \n",
    "vgg_activations_content = VGGActivations_content(renamed_model, content_layers_test)\n",
    "\n",
    "# Get the activations at each layer of the list\n",
    "with torch.no_grad():\n",
    "    activations_content = vgg_activations_content(input_image)\n",
    "\n",
    "reconstructed_images_content = reconstruct_image_content(activations_dict = activations_content)\n",
    "\n",
    "for layer, img in reconstructed_images_content.items():\n",
    "    \n",
    "    # Deprocess the image\n",
    "    output_image = deprocess(img.detach())\n",
    "    \n",
    "    # Display the image\n",
    "    image_display(output_image, layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8398f4ae-2688-46d0-b072-4e442deae027",
   "metadata": {},
   "source": [
    "<h2>2. Justifying the choice of the layers for style reconstruction</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c37a5d-28c2-42f6-a740-d117ca761a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "style_layers_test = [\"conv1_1\",\"conv2_1\",\"conv3_1\",\"conv4_1\",\"conv4_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4377c3b-ed40-40dc-9896-171a11a324f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGActivationsStyle(nn.Module):\n",
    "    \"\"\"\n",
    "    Extracts activations from specific layers of a VGG model and computes their Gram matrices \n",
    "    for style transfer tasks, returning a structured dictionary.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, target_layers):\n",
    "        \"\"\"\n",
    "        Initializes the class with the given model and the list of target layers.\n",
    "        \"\"\"\n",
    "        super(VGGActivationsStyle, self).__init__()\n",
    "        self.model = model\n",
    "        self.target_layers = target_layers\n",
    "        self.layer_outputs = {}\n",
    "\n",
    "    def gram_matrix(self, activation):\n",
    "        \"\"\"\n",
    "        Computes the Gram matrix of the activation to capture style information.\n",
    "        \"\"\"\n",
    "        a, b, c, d = activation.size()  \n",
    "        features = activation.view(a * b, c * d)  # Flatten spatial dimensions\n",
    "        G = torch.mm(features, features.t())  # Compute the dot product of feature maps\n",
    "        return G.div(a * b * c * d)  # Normalize by the total number of elements\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Passes input through the model to compute and store the Gram matrices \n",
    "        for the target layers, building a structured dictionary.\n",
    "        \"\"\"\n",
    "        self.layer_outputs = {}\n",
    "        model = nn.Sequential()\n",
    "        cumulative_grams = []  # List to store cumulative Gram matrices\n",
    "\n",
    "        for name, layer in self.model.named_children():\n",
    "            x = layer(x)  # Forward pass through the layer\n",
    "            if name in self.target_layers:\n",
    "                gram = self.gram_matrix(x)\n",
    "                cumulative_grams.append(gram)  # Add current Gram matrix\n",
    "                self.layer_outputs[name] = cumulative_grams.copy()  # Store copy of cumulative list\n",
    "\n",
    "        return self.layer_outputs\n",
    "\n",
    "\n",
    "\n",
    "def reconstruct_image_style(activations_dict):\n",
    "    \"\"\"\n",
    "    Reconstructs an image from the Gram matrices of activations using optimization.\n",
    "\n",
    "    Parameters:\n",
    "    ------------\n",
    "        activations_dict (dict): A dictionary containing the Gram matrices of activations for target layers.\n",
    "\n",
    "    Returns:\n",
    "    ------------\n",
    "        dict\n",
    "        A dictionary mapping each layer combination to the reconstructed image.\n",
    "    \"\"\"\n",
    "\n",
    "    reconstructed_images = {}\n",
    "\n",
    "    for layer in style_layers_test:\n",
    "        print(f\"{'-'*10} Running optimization for model up to layer: {layer} {'-'*10}\")\n",
    "\n",
    "        # Initialize a random image to optimize\n",
    "        reconstructed_image = torch.rand_like(input_image, requires_grad=True)\n",
    "\n",
    "        # Set up an optimizer for the image\n",
    "        optimizer = torch.optim.Adam([reconstructed_image], lr=0.01)\n",
    "\n",
    "        # Optimization loop\n",
    "        for step in range(3000):\n",
    "            optimizer.zero_grad()  # Reset gradients\n",
    "            loss = 0\n",
    "\n",
    "            # Compute activations for the reconstructed image\n",
    "            generated_activations = vgg_activationsStyle(reconstructed_image)\n",
    "\n",
    "            # Calculate loss for all target layers up to the current one\n",
    "            for prev_layer in style_layers_test[: style_layers_test.index(layer) + 1]:\n",
    "                target_gram = activations_dict[prev_layer][style_layers_test.index(prev_layer)]\n",
    "                generated_gram = generated_activations[prev_layer][style_layers_test.index(prev_layer)]\n",
    "                loss += torch.nn.functional.mse_loss(generated_gram, target_gram)\n",
    "\n",
    "            loss /= len(style_layers_test[: style_layers_test.index(layer) + 1])  # Normalize loss\n",
    "\n",
    "            loss.backward()  # Backpropagate the loss\n",
    "            optimizer.step()  # Update the image\n",
    "\n",
    "            if step % 50 == 0:\n",
    "                print(f\"Step {step}, Loss: {loss.item()}\")\n",
    "\n",
    "        reconstructed_images[layer] = reconstructed_image\n",
    "\n",
    "    return reconstructed_images\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196d15b2-df99-4906-af89-19bda0ef105d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prétraitement pour les images VGG\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "input_image = preprocess(torchvision.transforms.functional.to_pil_image(style_image.squeeze(0))).unsqueeze(0)\n",
    "\n",
    "# Instantiate the new model \n",
    "vgg_activations_style = VGGActivationsStyle(renamed_model, style_layers_test)\n",
    "\n",
    "# Get the activations at each layer of the list\n",
    "with torch.no_grad():\n",
    "    activations_style = vgg_activations_style(input_image)\n",
    "\n",
    "reconstructed_images_style = reconstruct_image_style(activations_dict = activations_style)\n",
    "\n",
    "for layer, img in reconstructed_images_style.items():\n",
    "    \n",
    "    # Deprocess the image\n",
    "    output_image = deprocess(img.detach())\n",
    "    \n",
    "    # Display the image\n",
    "    image_display(output_image, layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7520328-aca6-43ff-84d8-1a61decc95e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_dla",
   "language": "python",
   "name": "project_dla"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
