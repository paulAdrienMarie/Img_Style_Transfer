{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc11765e-e881-4f59-a038-badfb5131eec",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h1>Projet DLA</h1>\n",
    "    <h2>Style Transfer Using Convolutional Neural Network</h2>   \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed2ea10-898c-46f4-a265-bbb9a5b5e1b5",
   "metadata": {},
   "source": [
    "## Import modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb3630b-85ec-4f8b-b40a-6f93af0b08e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from torchvision.models import vgg19, VGG19_Weights\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fae5f8-b27f-4bc9-8e4e-fd9e5a216ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_loader(image_name):\n",
    "    # scale imported image\n",
    "    # transform it into a torch tensor\n",
    "    loader = transforms.Compose([transforms.Resize(imshape),  transforms.ToTensor()])\n",
    "\n",
    "    image = Image.open(image_name)\n",
    "    image = loader(image).unsqueeze(0)   # add an additional dimension for fake batch (here 1)\n",
    "    return image.to(device, torch.float) # move the image tensor to the correct device\n",
    "\n",
    "def image_display(tensor, title=None):\n",
    "    unloader = transforms.ToPILImage()  # reconvert into PIL image\n",
    "    image = tensor.cpu().clone()        # clone the tensor\n",
    "    image = unloader(image.squeeze(0))  # remove the fake batch dimension\n",
    "    plt.show()\n",
    "    plt.imshow(image)\n",
    "    if title is not None:\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a987292-efbe-40cd-8cf9-6a7328bedb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of the output image\n",
    "imshape = (224, 224)\n",
    "\n",
    "image_path = \"../data/\"\n",
    "content_image_name = \"content.jpeg\"\n",
    "style_image_name = \"style.jpeg\"\n",
    "\n",
    "content_image = image_loader(image_path + content_image_name)\n",
    "style_image = image_loader(image_path + style_image_name)\n",
    "\n",
    "content_height, content_width = content_image.shape[2], content_image.shape[3]\n",
    "style_height, style_width = style_image.shape[2], style_image.shape[3]\n",
    "\n",
    "print(f\"Content image shape : {content_height} x {content_width}\")\n",
    "print(f\"Style image shape : {style_height} x {style_width}\")\n",
    "print(content_image.size())\n",
    "image_display(content_image, \"Content image\")\n",
    "image_display(style_image, \"Style image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3a1586-fbde-403c-8611-a154cdd5863a",
   "metadata": {},
   "source": [
    "### Load the VGG-19 pretrained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c7e004-de12-4144-855c-d085af545626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the VGG 19 model with pre-trained weights\n",
    "model = models.vgg19(weights=VGG19_Weights.IMAGENET1K_V1).to(device) # move the model to the correct device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82c970b-76df-4d12-a7c3-b015f1473dbc",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h3>Justifying the choice of the layers for content reconstruction</h3>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dad6c7-9a03-490c-b343-f2ed235b3f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_layers_test = [\"conv1_2\",\"conv2_2\",\"conv3_2\",\"conv4_2\",\"conv5_2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d1acaf-82f2-4862-bfc9-fd149b503e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = [2, 2, 4, 4, 4]  # Number of convolutional layers in each block of the VGG-19 model\n",
    "renamed_model = nn.Sequential()\n",
    "\n",
    "# Renommer les couches\n",
    "index_conv = 0\n",
    "index_relu = 0\n",
    "current_block = 0\n",
    "i = 0\n",
    "\n",
    "for layer in model.features.eval().children():\n",
    "    # Vérifier si on doit passer au bloc suivant\n",
    "    if current_block < len(blocks) and index_conv == blocks[current_block]:\n",
    "        index_conv = 0  # Réinitialiser le compteur pour le nouveau bloc\n",
    "        current_block += 1\n",
    "\n",
    "    if isinstance(layer, nn.Conv2d):  # Pour les couches convolutionnelles\n",
    "        index_conv += 1\n",
    "        name = f'conv{current_block + 1}_{index_conv}'\n",
    "        renamed_model.add_module(name, layer)\n",
    "\n",
    "    elif isinstance(layer, nn.ReLU):  # Pour les couches ReLU\n",
    "        index_relu += 1\n",
    "        name = f'relu{current_block + 1}_{index_relu}'\n",
    "        renamed_model.add_module(name, nn.ReLU(inplace=False))  # Utiliser inplace=False pour compatibilité\n",
    "\n",
    "    elif isinstance(layer, nn.MaxPool2d):  # Pour les couches MaxPooling\n",
    "        name = f'pool{current_block + 1}'\n",
    "        renamed_model.add_module(name, layer)\n",
    "    i += 1\n",
    "# Afficher les premières couches pour vérifier les noms\n",
    "print(renamed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131d57c1-4855-4010-8965-944d896f6754",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGActivations_content(nn.Module):\n",
    "    \n",
    "    def __init__(self, model, target_layers):\n",
    "        \n",
    "        super(VGGActivations_content, self).__init__()\n",
    "        self.model = model\n",
    "        self.target_layers = target_layers\n",
    "        self.layer_outputs = {}\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.layer_outputs = {}\n",
    "        for name, layer in self.model.named_children():\n",
    "            x = layer(x)\n",
    "            if name in self.target_layers:\n",
    "                self.layer_outputs[name] = x\n",
    "        return self.layer_outputs\n",
    "\n",
    "def reconstruct_image_content(activations_dict):\n",
    "\n",
    "    # Dictionnary to store the generated images for each layer\n",
    "    reconstructed_images = {}\n",
    "    \n",
    "    for layer, activation in activations_dict.items():\n",
    "        print(f\"{'-'*10}Running optimization for layer : {layer} {'-'*10}\")\n",
    "        # Initialize the random image to optimize\n",
    "        reconstructed_image = torch.rand_like(input_image, requires_grad=True)\n",
    "\n",
    "        # Set the optimizer on the image on which we will perform gradient on\n",
    "        optimizer = torch.optim.Adam([reconstructed_image], lr=0.01)\n",
    "\n",
    "        # Loop to generate the target image\n",
    "        for step in range(3000):\n",
    "            # Reset the gradient to zero\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Get the activations of the corresponding layer\n",
    "            generated_activations = vgg_activations_content(reconstructed_image)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = torch.nn.functional.mse_loss(generated_activations[layer], activations_dict[layer])\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "\n",
    "            # Make a step for the gradient descent\n",
    "            optimizer.step()\n",
    "\n",
    "            # Display the loss\n",
    "            if step % 50 == 0:\n",
    "                print(f\"Step {step} - Loss: {loss.item()}\")\n",
    "\n",
    "        # Store the generated image for the corresponding layer\n",
    "        reconstructed_images[layer] = reconstructed_image\n",
    "\n",
    "    return reconstructed_images\n",
    "\n",
    "# Post-process for visualisation purposes\n",
    "def deprocess(tensor):\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)\n",
    "    tensor = tensor * std + mean  # Dénormalisation\n",
    "    return tensor.clamp(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a789123e-4e11-41ea-afd7-6bda85c7c387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prétraitement pour les images VGG\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "input_image = preprocess(torchvision.transforms.functional.to_pil_image(content_image.squeeze(0))).unsqueeze(0)\n",
    "\n",
    "# Instantiate the new model \n",
    "vgg_activations_content = VGGActivations_content(renamed_model, content_layers_test)\n",
    "\n",
    "# Get the activations at each layer of the list\n",
    "with torch.no_grad():\n",
    "    activations_content = vgg_activations_content(input_image)\n",
    "\n",
    "reconstructed_images_content = reconstruct_image_content(activations_dict = activations_content)\n",
    "\n",
    "for layer, img in reconstructed_images_content.items():\n",
    "    \n",
    "    # Deprocess the image\n",
    "    output_image = deprocess(img.detach())\n",
    "    \n",
    "    # Display the image\n",
    "    image_display(output_image, layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8398f4ae-2688-46d0-b072-4e442deae027",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <h3>Justifying the choice of the layers for style reconstruction</h3>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c37a5d-28c2-42f6-a740-d117ca761a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "style_layers_test = [\"conv1_1\",\"conv2_1\",\"conv3_1\",\"conv4_1\",\"conv4_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4377c3b-ed40-40dc-9896-171a11a324f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGActivations_style(nn.Module):\n",
    "    \n",
    "    def __init__(self, model, target_layers):\n",
    "        \n",
    "        super(VGGActivations_style, self).__init__()\n",
    "        self.model = model\n",
    "        self.target_layers = target_layers\n",
    "        self.layer_outputs = {}\n",
    "\n",
    "    def gram_matrix(self, activation):\n",
    "        \"\"\"Compute the gram matrix of the activation\"\"\"\n",
    "        a, b, c, d = activation.size()  \n",
    "        # a = batch size, \n",
    "        # b = number of feature maps, \n",
    "        # c, d = height, width\n",
    "        features = activation.view(a * b, c * d)   # flatten the feature maps\n",
    "        G = torch.mm(features, features.t())  # compute the dot product of the feature maps\n",
    "        return G.div(a * b * c * d)           # normalize the Gram matrix\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.layer_outputs = {}\n",
    "\n",
    "        # Create a new empty model\n",
    "        model = nn.Sequential()\n",
    "\n",
    "        # Initialize key name\n",
    "        key_name = \"\"\n",
    "        \n",
    "        # Loop over the different combinaisons of layers e.g the number of layers in the list, we add one layer at each iteration\n",
    "        for i in range(len(self.target_layers)):\n",
    "            \n",
    "            # Loop over the layers of the original model\n",
    "            for name, layer in self.model.named_children():\n",
    "                \n",
    "                if name == self.target_layers[i]:\n",
    "\n",
    "                    if key_name == \"\":\n",
    "                        key_name = name\n",
    "                    else:\n",
    "                        key_name += f\";{name}\"\n",
    "                    model.add_module(name,layer)\n",
    "                    \n",
    "            self.layer_outputs[key_name] = self.gram_matrix(model(x))\n",
    "        \n",
    "        return self.layer_outputs\n",
    "\n",
    "def reconstruct_image_style(activations_dict):\n",
    "\n",
    "    # Dictionnary to store the generated images for each layer\n",
    "    reconstructed_images = {}\n",
    "    \n",
    "    for layer, activation in activations_dict.items():\n",
    "        print(f\"{'-'*10}Running optimization for model with layers : {layer} {'-'*10}\")\n",
    "\n",
    "    \n",
    "        # Initialize the random image to optimize\n",
    "        reconstructed_image = torch.rand_like(input_image, requires_grad=True)\n",
    "\n",
    "        # Set the optimizer on the image on which we will perform gradient on\n",
    "        optimizer = torch.optim.Adam([reconstructed_image], lr=0.01)\n",
    "\n",
    "        # Loop to generate the target image\n",
    "        for step in range(3000):\n",
    "            # Reset the gradient to zero\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Get the activations of the corresponding layer\n",
    "            generated_activations = vgg_activations_style(reconstructed_image)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = torch.nn.functional.mse_loss(generated_activations[layer], activations_dict[layer])\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "\n",
    "            # Make a step for the gradient descent\n",
    "            optimizer.step()\n",
    "\n",
    "            # Display the loss\n",
    "            if step % 50 == 0:\n",
    "                print(f\"Step {step}, Loss: {loss.item()}\")\n",
    "\n",
    "        # Store the generated image for the corresponding layer\n",
    "        reconstructed_images[layer] = reconstruct_image\n",
    "\n",
    "    return reconstructed_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196d15b2-df99-4906-af89-19bda0ef105d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prétraitement pour les images VGG\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "input_image = preprocess(torchvision.transforms.functional.to_pil_image(style_image.squeeze(0))).unsqueeze(0)\n",
    "\n",
    "# Instantiate the new model \n",
    "vgg_activations_style = VGGActivations_style(renamed_model, style_layers_test)\n",
    "\n",
    "# Get the activations at each layer of the list\n",
    "with torch.no_grad():\n",
    "    activations_style = vgg_activations_style(input_image)\n",
    "\n",
    "reconstructed_images_style = reconstruct_image_style(activations_dict = activations_style)\n",
    "\n",
    "for layer, img in reconstructed_images_style.items():\n",
    "    \n",
    "    # Deprocess the image\n",
    "    output_image = deprocess(img.detach())\n",
    "    \n",
    "    # Display the image\n",
    "    image_display(output_image, layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7520328-aca6-43ff-84d8-1a61decc95e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_dla",
   "language": "python",
   "name": "project_dla"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
