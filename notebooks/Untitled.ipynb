{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da1350bc-a822-44b1-8f07-6ea4acdee285",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b70c287-9d07-4cb0-85b1-ca72480863af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /Users/paul-adrienmarie/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 548M/548M [07:20<00:00, 1.30MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Load the pretrained model\n",
    "vgg = models.vgg19(weights='DEFAULT').features\n",
    "\n",
    "# Freeze model parameters\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Transformation to preprocess images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11340181-1f03-45e4-8598-b4af7cdbb4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(img_path):\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0)\n",
    "    return image\n",
    "\n",
    "# Content and style images\n",
    "content_img = load_image(\"../data/content.jpeg\")\n",
    "style_img = load_image(\"../data/style.jpeg\")\n",
    "\n",
    "# Helper function to calculate content loss\n",
    "def content_loss(target, content):\n",
    "    return torch.mean((target - content) ** 2)\n",
    "\n",
    "# Helper function to calculate style loss\n",
    "def gram_matrix(tensor):\n",
    "    _, d, h, w = tensor.size()\n",
    "    tensor = tensor.view(d, h * w)\n",
    "    return torch.mm(tensor, tensor.t()) / (d * h * w)\n",
    "\n",
    "def style_loss(target, style):\n",
    "    target_gram = gram_matrix(target)\n",
    "    style_gram = gram_matrix(style)\n",
    "    return torch.mean((target_gram - style_gram) ** 2)\n",
    "\n",
    "# Define layers to capture\n",
    "content_layers = ['conv_4']\n",
    "style_layers = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n",
    "\n",
    "# Extract features from layers\n",
    "def get_features(image, model):\n",
    "    features = {}\n",
    "    x = image\n",
    "    for name, layer in model._modules.items():\n",
    "        x = layer(x)\n",
    "        if f\"conv_{len(features)+1}\" in content_layers + style_layers:\n",
    "            features[f\"conv_{len(features)+1}\"] = x\n",
    "    return features\n",
    "\n",
    "# Optimized input image\n",
    "input_img = content_img.clone().requires_grad_(True)\n",
    "optimizer = optim.LBFGS([input_img])\n",
    "\n",
    "# Define loss weights\n",
    "style_weight = 1e6\n",
    "content_weight = 1\n",
    "\n",
    "# Optimization\n",
    "iterations = 300\n",
    "for i in range(iterations):\n",
    "    def closure():\n",
    "        input_img.data.clamp_(0, 1)  # Clamp to valid range\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Get features\n",
    "        input_features = get_features(input_img, vgg)\n",
    "        content_features = get_features(content_img, vgg)\n",
    "        style_features = get_features(style_img, vgg)\n",
    "\n",
    "        # Calculate content loss\n",
    "        c_loss = content_weight * content_loss(input_features['conv_4'], content_features['conv_4'])\n",
    "\n",
    "        # Calculate style loss\n",
    "        s_loss = 0\n",
    "        for layer in style_layers:\n",
    "            s_loss += style_loss(input_features[layer], style_features[layer])\n",
    "        s_loss *= style_weight\n",
    "\n",
    "        # Total loss\n",
    "        total_loss = c_loss + s_loss\n",
    "        total_loss.backward()\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    optimizer.step(closure)\n",
    "\n",
    "# Convert tensor to image\n",
    "output_img = input_img.clone().detach().cpu().squeeze()\n",
    "output_img = transforms.ToPILImage()(output_img.clamp_(0, 1))\n",
    "output_img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "613842f9-6723-4107-9f5f-e1e749250cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1657a864-3a26-4516-9024-67c777caea49",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "dirname() missing 1 required positional argument: 'p'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdirname\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: dirname() missing 1 required positional argument: 'p'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cd563e-744b-4a50-a86e-7baaae999e95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_dla",
   "language": "python",
   "name": "project_dla"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
